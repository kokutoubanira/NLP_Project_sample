{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "text_classification.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kokutoubanira/NLP_Project_sample/blob/main/text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nby-9Ofi9NTA"
      },
      "source": [
        "Google Colaboratoryでivedoorニュース9カテゴリを分類する自然言語処理の実装について解説します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKOuuM0o9NTD"
      },
      "source": [
        "※　本章のファイルはすべてUbuntuでの動作を前提としています。Windowsなど文字コードが違う環境での動作にはご注意下さい。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1FGJtHJ9NTD"
      },
      "source": [
        "本実装の流れは、\n",
        "\n",
        "1. livedoorニュースをダウンロードして、tsvファイルに変換\n",
        "2. tsvファイルをPyTorchのtorchtextのDataLoaderに変換\n",
        "3. クラス分類用のモデルを用意する\n",
        "4. パラメータの設定\n",
        "5. 学習の実施\n",
        "6. テストデータでの性能を確認"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEQijNwScYne"
      },
      "source": [
        "1. livedoorニュースをダウンロードして、tsvファイルに変換\n",
        "ここでは、livedoorニュースをダウンロードし、\n",
        "\n",
        "本文[tab]クラスラベル\n",
        "本文[tab]クラスラベル\n",
        "本文[tab]クラスラベル\n",
        "\n",
        "の構成のtsvファイルへと変換していきます。\n",
        "\n",
        "まず、LiveDoorニュースをダウンロードします（乱数のシード固定は掲載を省略しています）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85X58Qil9NTE"
      },
      "source": [
        "### Janomeのインストール方法\n",
        "\n",
        "コンソールにて、\n",
        "\n",
        "- source activate pytorch_p36\n",
        "- pip install janome"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "301JGsx1ITeO"
      },
      "source": [
        "!pip install janome"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3fiaR4ucd5b"
      },
      "source": [
        "# Livedoorニュースのファイルをダウンロード\n",
        "! wget \"https://www.rondhuit.com/download/ldcc-20140209.tar.gz\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTFR4Guichsl"
      },
      "source": [
        "ダウンロードした圧縮ファイルを解凍しカテゴリーの数と内容を確認します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnhxrcdFdQ_Q"
      },
      "source": [
        "# ファイルを解凍し、カテゴリー数と内容を確認\n",
        "import tarfile\n",
        "import os\n",
        "\n",
        "# 解凍\n",
        "tar = tarfile.open(\"ldcc-20140209.tar.gz\", \"r:gz\")\n",
        "tar.extractall(\"./data/livedoor/\")\n",
        "tar.close()\n",
        "\n",
        "# フォルダのファイルとディレクトリを確認\n",
        "files_folders = [name for name in os.listdir(\"./data/livedoor/text/\")]\n",
        "print(files_folders)\n",
        "\n",
        "# カテゴリーのフォルダのみを抽出\n",
        "categories = [name for name in os.listdir(\n",
        "    \"./data/livedoor/text/\") if os.path.isdir(\"./data/livedoor/text/\"+name)]\n",
        "\n",
        "print(\"カテゴリー数:\", len(categories))\n",
        "print(categories)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3_SDttudYeo"
      },
      "source": [
        "カテゴリーではない、ファイルなどもあるので、それを無視します。\n",
        "\n",
        "ひとつ、ファイルの中身を確認してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPhCo05IdbI3"
      },
      "source": [
        "# ファイルの中身を確認してみる\n",
        "file_name = \"./data/livedoor/text/movie-enter/movie-enter-6255260.txt\"\n",
        "\n",
        "with open(file_name) as text_file:\n",
        "    text = text_file.readlines()\n",
        "    print(\"0：\", text[0])  # URL情報\n",
        "    print(\"1：\", text[1])  # タイムスタンプ\n",
        "    print(\"2：\", text[2])  # タイトル\n",
        "    print(\"3：\", text[3])  # 本文\n",
        "\n",
        "    # 今回は4要素目には本文は伸びていないが、4要素目以降に本文がある場合もある"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uj5Z82fedezN"
      },
      "source": [
        "今回は、4要素目に本文が入っていませんが、4要素目以降に本文が入っている場合もあります。\n",
        "\n",
        "この各ファイルから、タイトルは除いて、本文だけを抽出したtsvファイルを作成したいです。\n",
        "\n",
        "タイトルの除くのは、タイトルは文章内容の要約であり、さすがにクラス分類のための情報量が多すぎるからです。\n",
        "\n",
        "本文を取得する前処理関数を定義します。ここでは改行や全角スペースも削除しています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuIzIMYgeeqD"
      },
      "source": [
        "# 本文を取得する前処理関数を定義\n",
        "\n",
        "\n",
        "def extract_main_txt(file_name):\n",
        "    with open(file_name) as text_file:\n",
        "        # 今回はタイトル行は外したいので、3要素目以降の本文のみ使用\n",
        "        text = text_file.readlines()[3:]\n",
        "\n",
        "        # 3要素目以降にも本文が入っている場合があるので、リストにして、後で結合させる\n",
        "        text = [sentence.strip() for sentence in text]  # 空白文字(スペースやタブ、改行)の削除\n",
        "        text = list(filter(lambda line: line != '', text))\n",
        "        text = ''.join(text)\n",
        "        text = text.translate(str.maketrans(\n",
        "            {'\\n': '', '\\t': '', '\\r': '', '\\u3000': ''}))  # 改行やタブ、全角スペースを消す\n",
        "        return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMNFjapieW2s"
      },
      "source": [
        "この定義した前処理関数を利用して、全ファイルを変換します。\n",
        "livedoorニュースの9カテゴリについて、各カテゴリーごとに処理を実施します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6dqf2CZeZZg"
      },
      "source": [
        "# リストに前処理した本文と、カテゴリーのラベルを追加していく\n",
        "import glob\n",
        "\n",
        "list_text = []\n",
        "list_label = []\n",
        "\n",
        "for cat in categories:\n",
        "    text_files = glob.glob(os.path.join(\"./data/livedoor/text\", cat, \"*.txt\"))\n",
        "\n",
        "    # 前処理extract_main_txtを実施して本文を取得\n",
        "    body = [extract_main_txt(text_file) for text_file in text_files]\n",
        "\n",
        "    label = [cat] * len(body)  # bodyの数文だけカテゴリー名のラベルのリストを作成\n",
        "\n",
        "    list_text.extend(body)  # appendが要素を追加するのに対して、extendはリストごと追加する\n",
        "    list_label.extend(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TA3crzlnw5y"
      },
      "source": [
        "リストをpandasのDataFrameに変換します。サイズを確認すると、7,376の文章があることが確認できます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0lcVrL7n3Vq"
      },
      "source": [
        "# pandasのDataFrameにする\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'text': list_text, 'label': list_label})\n",
        "\n",
        "# 大きさを確認しておく（7,376文章が存在）\n",
        "print(df.shape)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7kCSuVun9Up"
      },
      "source": [
        "続いて、カテゴリー名を数値に変換する辞書を作成します。\n",
        "そして、その辞書で数値に置き換えたDataFrameを用意します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvmrAp9moDYA"
      },
      "source": [
        "# カテゴリーの辞書を作成\n",
        "dic_id2cat = dict(zip(list(range(len(categories))), categories))\n",
        "dic_cat2id = dict(zip(categories, list(range(len(categories)))))\n",
        "\n",
        "print(dic_id2cat)\n",
        "print(dic_cat2id)\n",
        "\n",
        "# DataFrameにカテゴリーindexの列を作成\n",
        "df[\"label_index\"] = df[\"label\"].map(dic_cat2id)\n",
        "df.head()\n",
        "\n",
        "# label列を消去し、text, indexの順番にする\n",
        "df = df.loc[:, [\"text\", \"label_index\"]]\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKRImxI_oJWv"
      },
      "source": [
        "データがシャッフルされておらず、カテゴリーごとに固まっているので、シャッフルします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwL2LXvdoLTy"
      },
      "source": [
        "# 順番をシャッフルする\n",
        "df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6ire4taoNsp"
      },
      "source": [
        "シャッフルされたデータの前2割をテストデータ、残りの8割は訓練&検証データとします。\n",
        "\n",
        "結果、テストデータが1,475件、訓練&検証データが5,901件となります。\n",
        "\n",
        "これをtest.tsv、train_eval.tsvとして、それぞれ保存します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAficu3goP2I"
      },
      "source": [
        "# tsvファイルで保存する\n",
        "\n",
        "# 全体の2割の文章数\n",
        "len_0_2 = len(df) // 5\n",
        "\n",
        "# 前から2割をテストデータとする\n",
        "df[:len_0_2].to_csv(\"./test.tsv\", sep='\\t', index=False, header=None)\n",
        "print(df[:len_0_2].shape)\n",
        "\n",
        "# 前2割からを訓練&検証データとする\n",
        "df[len_0_2:].to_csv(\"./train_eval.tsv\", sep='\\t', index=False, header=None)\n",
        "print(df[len_0_2:].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASLjJuT-oSIx"
      },
      "source": [
        "以上でlivedoorニュースのデータをtsvファイルに変換することができました。\n",
        "\n",
        "なお、tsvファイルをGoogle Colaboratoryからダウンロードしたい場合は以下のfile.downloadのコメントを外して実行します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIeoK04AoTmI"
      },
      "source": [
        "# tsvファイルをダウンロードしたい場合\n",
        "from google.colab import files\n",
        "\n",
        "# ダウンロードする場合はコメントを外す\n",
        "# 少し時間がかかる（4MB）\n",
        "# files.download(\"./test.tsv\")\n",
        "\n",
        "\n",
        "# ダウンロードする場合はコメントを外す\n",
        "# 少し時間がかかる（18MB）\n",
        "# files.download(\"./train_eval.tsv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_exy3cGxrYn"
      },
      "source": [
        "2. tsvファイルをPyTorchのtorchtextのDataLoaderに変換"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fG3dbqR-x23I"
      },
      "source": [
        "続いて、作成したtsvファイルを、PyTorchで扱えるDataLoaderに変換します。\n",
        "torchtextを使用します。\n",
        "形態素解析のMeCabをインストールします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ-y3iZ-yCzf"
      },
      "source": [
        "# MeCabとtransformersの用意\n",
        "!apt install aptitude swig\n",
        "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "!pip install mecab-python3\n",
        "!pip install transformers==2.9.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SEP9_nx9NTF"
      },
      "source": [
        "from janome.tokenizer import Tokenizer\n",
        "\n",
        "j_t = Tokenizer()\n",
        "\n",
        "text = '機械学習が好きです。'\n",
        "\n",
        "for token in j_t.tokenize(text):\n",
        "    print(token)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nziuJKGE9NTG"
      },
      "source": [
        "# 単語分割する関数を定義\n",
        "\n",
        "\n",
        "def tokenizer_janome(text):\n",
        "    return [tok for tok in j_t.tokenize(text, wakati=True)]\n",
        "\n",
        "\n",
        "text = '機械学習が好きです。'\n",
        "print(tokenizer_janome(text))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m29MrBUu9NTH"
      },
      "source": [
        "MeCab\n",
        "\n",
        "公式サイト\n",
        "\n",
        "http://taku910.github.io/mecab/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLrUi3BP96nL"
      },
      "source": [
        "!pip install mecab-python3\n",
        "!sudo apt install libmecab-dev\n",
        "!sudo apt install mecab-ipadic-utf8\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0pitsgHydS1"
      },
      "source": [
        "モデル（分類タスク用）の実装\n",
        "本ファイルでは、クラス分類のモデルを実装します。\n",
        "※　本章のファイルはすべてUbuntuでの動作を前提としています。Windowsなど文字コードが違う環境での動作にはご注意下さい。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFb5R-wNygQG"
      },
      "source": [
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch \n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import torchtext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7wagaxHy77r"
      },
      "source": [
        "# 乱数のシードを設定\n",
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)\n",
        "random.seed(1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lm8Nav23Cqj"
      },
      "source": [
        "形態素解析の準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiijUAU13ENt"
      },
      "source": [
        "形態素解析はpipで簡単にインストールできるmecabのラッパーであるfugashiを使います。\n",
        "下記のように辞書も一緒にpipでインストールできます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3P9w1vsy9S_"
      },
      "source": [
        "!pip install fugashi\n",
        "!pip install unidic-lite"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gxKlVvo3HJ1"
      },
      "source": [
        "形態素解析をする関数は以下のように特に前処理などは施さないシンプルなものにしました。\n",
        "fugashiは以下のように、mecabと同様の使い方ができます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSgByzQ23GOQ"
      },
      "source": [
        "import fugashi\n",
        "\n",
        "tagger = fugashi.Tagger(\"-Owakati\")\n",
        "def make_wakati(text):\n",
        "    text = tagger.parse(text)\n",
        "    wakati = text.split(\" \")\n",
        "    wakati = list(filter((\"\").__ne__, wakati))\n",
        "    return wakati\n",
        "\n",
        "# 形態素解析テスト\n",
        "text = df.sample(n=1)['text'].item()\n",
        "print(make_wakati(text)[:30])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUa4XLO43ydY"
      },
      "source": [
        "livedoorニュースコーパスを学習データ、検証データ、テストデータの3つに分割しています。\n",
        "CNNの実装を確かめることがメインなので、単語ベクトルも今回はとりあえず学習データからvocabularyを生成して、ランダムなベクトルを扱うことにします。また、文章の最大長を指定するmax_lengthなどは特に指定していません。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3ScXmk54w6V"
      },
      "source": [
        "livedoor_df = df.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiUyu6dg3yKW"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from torchtext.legacy import data\n",
        "\n",
        "# カテゴリーをidに変換します。\n",
        "categories = livedoor_df['label_index'].unique().tolist()\n",
        "livedoor_df['category_id'] = livedoor_df['label_index'].map(lambda x: categories.index(x))\n",
        "\n",
        "# 元データを学習、検証、テストの３つに分割します。\n",
        "train_val_df, test_df = train_test_split(livedoor_df[['text', 'label_index']], train_size=0.8)\n",
        "train_df, val_df = train_test_split(train_val_df, train_size=0.75)\n",
        "\n",
        "print('train size', train_df.shape)\n",
        "print('validation size', val_df.shape)\n",
        "print('test size', test_df.shape)\n",
        "# train size (4425, 2)\n",
        "# validation size (1475, 2)\n",
        "# test size (1476, 2)\n",
        "\n",
        "# torchtext用にtsvファイルで保存します。\n",
        "train_df.to_csv('train.tsv', sep='\\t', index=False, header=None)\n",
        "val_df.to_csv('val.tsv', sep='\\t', index=False, header=None)\n",
        "test_df.to_csv('test.tsv', sep='\\t', index=False, header=None)\n",
        "\n",
        "TEXT = data.Field(sequential=True, tokenize=make_wakati, lower=False, batch_first=True, pad_token='<pad>')\n",
        "LABEL = data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "train_data, val_data, test_data = data.TabularDataset.splits(\n",
        "    path=\"./\", train='train.tsv', validation='val.tsv', test='test.tsv', format='tsv', fields=[('Text', TEXT), ('Label', LABEL)])\n",
        "\n",
        "# vocabulary生成\n",
        "# 学習データだけでvocabを作成します。\n",
        "TEXT.build_vocab(train_data, min_freq=1)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "train_loader = data.Iterator(train_data, batch_size=BATCH_SIZE, train=True)\n",
        "val_loader = data.Iterator(val_data, batch_size=BATCH_SIZE, train=False, sort=False)\n",
        "test_loader = data.Iterator(test_data, batch_size=BATCH_SIZE, train=False, sort=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFaQzs1U5a8U"
      },
      "source": [
        "CNNによるモデルの定義"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Abfy-lwi5sQ3"
      },
      "source": [
        "自然言語処理におけるCNNの実装解説\n",
        "自然言語処理では文章を行列（単語ベクトルの集まり）として扱うことが多いですが、その行列を（チャネル1の）画像とみなせば、自然言語に対してCNNを適用することができます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POiOROpS5005"
      },
      "source": [
        "まずは文章の行列を用意します。要素がランダムなミニバッチサイズ2の7××5の行列を用意しています。\n",
        "自然言語処理で考えると、長さが7, 単語のベクトル次元数が5の文章を2つ用意した、ということになります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B44q6yrh3ih2"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "mat = torch.rand(2, 7, 5)\n",
        "print(mat.size())\n",
        "#torch.Size([2, 7, 5])\n",
        "print(mat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTPnJtDr57GL"
      },
      "source": [
        "この文章を畳み込みます。畳み込みフィルターのサイズは図の通り、4×5とし、ストライドは1とします。このフィルターを2枚畳み込みたいので、アウトプットのチャネルは2を指定すればOK。\n",
        "自然言語処理でnn.LSTMなどを扱うとき、インプットの形式は（batch_first=Trueを指定した場合）ミニバッチサイズ×文章の長さ×単語ベクトル次元数のテンソルを扱いますが、nn.Conv2dのインプットの形式はミニバッチサイズ×チャネル数×高さ×幅である必要があります。なので、下記のようにmat.unsqueeze(1)をしてミニバッチサイズの次にチャネル1の次元を追加しています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLJofDfk53W9"
      },
      "source": [
        "# 第１引数はインプットのチャネル（今回は1）を指定\n",
        "# 自然言語処理で畳み込む場合、異なる単語分散表現（word2vecとfasttextみたいな）などを使って、\n",
        "# 複数チャネルとみなす方法もあるようです。\n",
        "# 第２引数はアウトプットのチャネル数で、今回は同じフィルターを2枚畳み込みたいので、2を指定\n",
        "# カーネルサイズは高さ×幅を指定しており、幅は図で説明した通り、単語ベクトルの次元数5を指定\n",
        "conv = nn.Conv2d(1, 2, kernel_size=(4, 5))\n",
        "\n",
        "# チャネル数1を挿入\n",
        "mat = mat.unsqueeze(1)\n",
        "print(mat.size())\n",
        "# torch.Size([2, 1, 7, 5]) \n",
        "# ↑ミニバッチサイズ×チャネル数×文章の長さ×単語ベクトル次元数\n",
        "\n",
        "# 畳み込む\n",
        "feature = conv(mat)\n",
        "print(feature.size())\n",
        "# torch.Size([2, 2, 4, 1])\n",
        "# ↑ミニバッチサイズ×特徴マップの数×（特徴マップの形式4×1）\n",
        "print(feature)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lxw8EZtD6GuT"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "feature = F.relu(feature)\n",
        "print(feature.size())\n",
        "# ↑ミニバッチサイズ×特徴マップの数×（特徴マップの形式4×1）\n",
        "print(feature)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0gAUiW76Mrs"
      },
      "source": [
        "1-max poolingを行います。ここですることは要は各特徴マップの最大要素を抽出することになります。つまりnn.MaxPool2dを使って、"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27AgkILH6KKj"
      },
      "source": [
        "# nn.MaxPool1dでも良いですが、そのときは上のfeatureに対して\n",
        "# feauture.unsqueeze(-1)をして最後の次元の1を除去しましょう。\n",
        "pool = nn.MaxPool2d(kernel_size=(4, 1))\n",
        "print(pool(feature))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHhm_9PS6Sgu"
      },
      "source": [
        "としたくなりますが、上のpoolingのカーネルサイズ(上の4のところ)はインプットとなる特徴マップのサイズに依存するんですよね。特徴マップのサイズは元の文章の長さ（最初の行列の行方向）と畳み込みフィルターのサイズに依存するので、上のようにnn.MaxPool2dでpoolingするレイヤーのインスタンスを宣言しちゃうと、poolingのカーネルサイズを可変にできないので、F.max_pool2dを使って以下のようにpoolingする際、インプットとなる特徴マップのサイズを指定するようにします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSCv2F9a6PZr"
      },
      "source": [
        "# feature.size()[2]で特徴マップの高さを取得しています。\n",
        "feature = F.max_pool2d(feature, kernel_size=(feature.size()[2], 1))\n",
        "print(feature.size())\n",
        "# torch.Size([2, 2, 1, 1])\n",
        "print(feature)\n",
        "# viewを使って次元数を整頓します。\n",
        "feature = feature.view(-1, 2)\n",
        "print(feature.size())\n",
        "# torch.Size([2, 2])\n",
        "print(feature)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-J8JmGz6Yzk"
      },
      "source": [
        "あとは同様のことを異なる畳み込みフィルターにも適用して、最後に要素を結合して全結合層にぶち込めばOKですね。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpEAkZ-v6kFh"
      },
      "source": [
        "ネットワークの定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvXnQEyj6W_T"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(Net, self).__init__()\n",
        "        # 単語分散表現はランダムベクトルを使う\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=TEXT.vocab.stoi['<pad>'])\n",
        "        # 図の黄色い畳み込みフィルター\n",
        "        self.conv1 = nn.Conv2d(1, 2, kernel_size=(2, embedding_dim))\n",
        "        # 図の緑色の畳み込みフィルター\n",
        "        self.conv2 = nn.Conv2d(1, 2, kernel_size=(3, embedding_dim))\n",
        "        # 図の赤色の畳み込みフィルター\n",
        "        self.conv3 = nn.Conv2d(1, 2, kernel_size=(4, embedding_dim))\n",
        "\n",
        "        # 3つ畳み込みの処理でそれぞれ2次元のベクトルが生成されるので、それらを全て結合して6次元のベクトルとなります。\n",
        "        # livedoorのカテゴリは9つなので、アウトプットサイズは9を指定\n",
        "        self.linear = nn.Linear(6, 9)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        # ①文章の行列を取得\n",
        "        out = self.embeddings(input_ids)\n",
        "        # チャネル数1を挿入\n",
        "        out = out.unsqueeze(1)\n",
        "\n",
        "        # ②畳み込んでreluに通す\n",
        "        out1 = F.relu(self.conv1(out))\n",
        "        out2 = F.relu(self.conv2(out))\n",
        "        out3 = F.relu(self.conv3(out))\n",
        "\n",
        "        # ③poolingして、各特徴マップの最大要素を取得\n",
        "        out1 = F.max_pool2d(out1, kernel_size=(out1.size()[2], 1))\n",
        "        out2 = F.max_pool2d(out2, kernel_size=(out2.size()[2], 1))\n",
        "        out3 = F.max_pool2d(out3, kernel_size=(out3.size()[2], 1))\n",
        "\n",
        "        # ④viewして次元を整えてあげる\n",
        "        out1 = out1.view(-1, 2)\n",
        "        out2 = out2.view(-1, 2)\n",
        "        out3 = out3.view(-1, 2)\n",
        "\n",
        "        # ⑤全部結合して1本のベクトルにする\n",
        "        out = torch.cat([out1, out2, out3], dim=1)\n",
        "\n",
        "        # ⑥全結合層で９つのカテゴリー分類できるように変換\n",
        "        out = self.linear(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7LMW8k96ph7"
      },
      "source": [
        "学習\n",
        "あとはこのネットワークでちゃんと学習できるか確かめて精度を確認して終わりです。\n",
        "学習部分は以下のように実装しました。\n",
        "学習データ、検証データともに順調に損失は減っていきますが、検証データの損失が最後らへんで増えてしまいます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "th24llqc6nk5"
      },
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm_notebook as tqdm \n",
        "VOCAB_SIZE = len(TEXT.vocab.stoi)\n",
        "EMBEDDING_DIM = 200\n",
        "\n",
        "net = Net(VOCAB_SIZE, EMBEDDING_DIM)\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "# GPUの設定\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "# ネットワークをGPUへ送る\n",
        "net.to(device)\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "train_accuracy = []\n",
        "val_accuracy = []\n",
        "\n",
        "for epoch in tqdm(range(30)):\n",
        "\n",
        "    # 学習\n",
        "    _train_loss = 0.0\n",
        "    _train_acc = 0.0\n",
        "    net.train()\n",
        "    for batch in tqdm(train_loader):\n",
        "        inputs = batch.Text.to(device)\n",
        "        y = batch.Label.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = net(inputs)\n",
        "        loss = loss_function(out, y)\n",
        "        _, preds = torch.max(out, 1)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _train_loss += loss.item()\n",
        "        _train_acc += torch.sum(preds == y).item()\n",
        "    train_loss.append(_train_loss)\n",
        "    train_epoch_acc = _train_acc / len(train_loader.dataset)\n",
        "    train_accuracy.append(train_epoch_acc)\n",
        "\n",
        "    # 検証\n",
        "    _val_loss = 0.0\n",
        "    _val_acc = 0.0\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            inputs = batch.Text.to(device)\n",
        "            y = batch.Label.to(device)\n",
        "            out = net(inputs)\n",
        "            loss = loss_function(out, y)\n",
        "            _, preds = torch.max(out, 1)\n",
        "            _val_loss += loss.item()\n",
        "            _val_acc += torch.sum(preds == y).item()\n",
        "    val_loss.append(_val_loss)\n",
        "    val_epoch_acc = _val_acc / len(val_loader.dataset)\n",
        "    val_accuracy.append(val_epoch_acc)\n",
        "\n",
        "    print(\"epoch\", epoch,\n",
        "          \"\\ttrain loss\", round(_train_loss, 4), \"\\ttrain accuracy\", round(train_epoch_acc, 4),\n",
        "          \"\\tval loss\", round(_val_loss, 4), \"\\tval accuracy\", round(val_epoch_acc, 4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8-XCKbi6xKP"
      },
      "source": [
        "精度確認\n",
        "最後にテストデータによる精度（Fスコア）を確認しましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jixlNMmC6uhx"
      },
      "source": [
        "# 精度確認\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_loss = 0.0\n",
        "    net.eval()\n",
        "    prediction = []\n",
        "    answer = []\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch.Text.to(device)\n",
        "        y = batch.Label.to(device)\n",
        "        out = net(input_ids)\n",
        "        _, preds = torch.max(out, 1)\n",
        "        prediction += list(preds.cpu().numpy())\n",
        "        answer += list(y.cpu().numpy())\n",
        "print(classification_report(prediction, answer, target_names=categories))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGtVYT137N5P"
      },
      "source": [
        "おわりに\n",
        "自然言語処理に対するCNNの適用例ということでCNNによる文章分類の実装を確認しました。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KMULyHh9jeG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}