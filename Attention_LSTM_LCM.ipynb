{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn,optim\n",
    "from torch.utils.data import(Dataset, DataLoader, TensorDataset)\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "import re \n",
    "import collections\n",
    "import itertools\n",
    "import MeCab\n",
    "import neologdn\n",
    "import math\n",
    "import emoji\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors\n",
    "import glob\n",
    "import os\n",
    "import io\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "from time import sleep\n",
    "import csv\n",
    "import pandas as pd\n",
    "import torchnlp.metrics.bleu as bleu\n",
    "\n",
    "mecab = MeCab.Tagger('-Owakati -d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd')\n",
    "mecab.parse('')  # バグ対処"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_marks_regex = re.compile(\"[\\,\\(\\)\\[\\]\\*:;]|<.*?>\")\n",
    "shift_marks_regex = re.compile(\"([?!\\.])\")\n",
    "\n",
    "unk = 0\n",
    "sos = 1\n",
    "eos = 2\n",
    "\n",
    "def normalize(text):\n",
    "    text = text.lower()\n",
    "#     #不要な文字を削除\n",
    "#     text = remove_marks_regex.sub(\"\", text)\n",
    "    #?!.と単語の間に空白を挿入\n",
    "    text = shift_marks_regex.sub(r\"\\1\", text)\n",
    "    #重ね表現の削除\n",
    "    text = neologdn.normalize(text)\n",
    "    #url削除\n",
    "    text = re.sub(r'https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+', '', text)\n",
    "    #絵文字削除\n",
    "    text = ''.join(['' if c in emoji.UNICODE_EMOJI else c for c in text])\n",
    "    #桁区切りの削除\n",
    "    text = re.sub(r'(\\d)([,.])(\\d+)', r'\\1\\3', text)\n",
    "    text = re.sub(r'\\d+', '0', text)\n",
    "    # 半角記号の置換\n",
    "    text = re.sub(r'[!-/:-@[-`{-~]', r' ', text)\n",
    "    # 全角記号の置換 (ここでは0x25A0 - 0x266Fのブロックのみを除去)\n",
    "    text = re.sub(u'[■-♯]', ' ', text)\n",
    "    text = text.replace(\"【\", \" \").replace(\"】\", \" \").replace(\"『\",\" \").replace(\"』\", \" \").replace(\"、\", \" \").replace(\"。\", \" \").replace(\"”\", \" \").replace('\"', \" \")\n",
    "    return text\n",
    "\n",
    "def make_wakati(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    # MeCabで分かち書き\n",
    "    sentence = mecab.parse(sentence)\n",
    "    # 半角全角英数字除去\n",
    "    sentence = re.sub(r'[0-9０-９a-zA-Zａ-ｚＡ-Ｚ]+', \" \", sentence)\n",
    "    # 記号もろもろ除去\n",
    "    sentence = re.sub(r'[\\．_－―─！＠＃＄％＾＆\\-‐|\\\\＊\\“（）＿■×+α※÷⇒—●★☆〇◎◆▼◇△□(：〜～＋=)／*&^%$#@!~`){}［］…\\[\\]\\\"\\'\\”\\’:;<>?＜＞〔〕〈〉？、。・,\\./『』【】「」→←○《》≪≫\\n\\u3000]+', \"\", sentence)\n",
    "    # スペースで区切って形態素の配列へ\n",
    "    wakati = sentence.split(\" \")\n",
    "    # 空の要素は削除\n",
    "    wakati = list(filter((\"\").__ne__, wakati))\n",
    "    return wakati\n",
    "\n",
    "def parse_line(line):\n",
    "    src,trg = line.split(\"\\t\")#[:2]\n",
    "    #翻訳元と翻訳先それぞれのトークンリストを作成する\n",
    "#     src = mecab.parse(str(src))\n",
    "#     trg = mecab.parse(str(trg))\n",
    "#     src = make_wakati(str(src))\n",
    "#     trg = make_wakati(str(trg))\n",
    "    src_tokens = src.strip().split()\n",
    "    trg_tokens = trg.strip().split()\n",
    "    return src_tokens, trg_tokens\n",
    "#     return src_tokens, trg_tokens\n",
    "\n",
    "def build_vocab(tokens):\n",
    "    #ファイル中のすべての文章でのトークン数を数える\n",
    "    counts = collections.Counter(tokens)\n",
    "    #トークンの出現数の多い順に並べる\n",
    "    sorted_counts = sorted(counts.items(), key=lambda c: c[1], reverse=True)\n",
    "    #3つのタグを追加して正引きリストと逆引き用辞書を作る\n",
    "    word_list = [\"<UNK>\", \"<SOS>\", \"<EOS>\"] + [x[0] for x in sorted_counts]\n",
    "    word_dict = dict((w, i) for i, w in enumerate(word_list))\n",
    "    return word_list, word_dict\n",
    "\n",
    "def words2tensor(words, word_dict, max_len, padding=0):\n",
    "    #末尾に終了タグをつける\n",
    "    words = words + [\"<EOS>\"]\n",
    "    #辞書を利用して数値のリストに変換する\n",
    "    words = [word_dict.get(w,0) for w in words]\n",
    "    seq_len = len(words)\n",
    "    #長さがmax_len以下の場合はパディングする\n",
    "    if seq_len < max_len + 1:\n",
    "        words = words + [padding] * (max_len + 1 - seq_len)\n",
    "    #Tensorに変換して返す\n",
    "    return torch.tensor(words, dtype=torch.int64), seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != src.size(0):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(src.size(0)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        #output = self.decoder(output)\n",
    "        #######\n",
    "        #outputを線形層に置き換え\n",
    "        #######\n",
    "        \n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_marks_regex = re.compile(\"[\\,\\(\\)\\[\\]\\*:;]|<.*?>\")\n",
    "shift_marks_regex = re.compile(\"([?!\\.])\")\n",
    "\n",
    "def get_DataLoaders_and_TEXT(max_length=256, batch_size=24):\n",
    "    \"\"\"IMDbのDataLoaderとTEXTオブジェクトを取得する。 \"\"\"\n",
    "    def normalize(text):\n",
    "        text = text.lower()\n",
    "        #不要な文字を削除\n",
    "        text = remove_marks_regex.sub(\"\", text)\n",
    "        #?!.と単語の間に空白を挿入\n",
    "        text = shift_marks_regex.sub(r\"\\1\", text)\n",
    "        #重ね表現の削除\n",
    "        text = neologdn.normalize(text)\n",
    "        #url削除\n",
    "        text = re.sub(r'https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+', '', text)\n",
    "        #絵文字削除\n",
    "        text = ''.join(['' if c in emoji.UNICODE_EMOJI else c for c in text])\n",
    "        #桁区切りの削除\n",
    "        text = re.sub(r'(\\d)([,.])(\\d+)', r'\\1\\3', text)\n",
    "        text = re.sub(r'\\d+', '0', text)\n",
    "        # 半角記号の置換\n",
    "        text = re.sub(r'[!-/:-@[-`{-~]', r' ', text)\n",
    "        # 全角記号の置換 (ここでは0x25A0 - 0x266Fのブロックのみを除去)\n",
    "        text = re.sub(u'[■-♯]', ' ', text)\n",
    "        text = text.replace(\"【\", \"「\").replace(\"】\", \"」\").replace(\"『\",\"「\").replace(\"』\",\"」\")\n",
    "        return text\n",
    "\n",
    "    def preprocessing_text(text):\n",
    "        # 改行コードを消去\n",
    "        text = re.sub('<br />', '', text)\n",
    "\n",
    "        # カンマ、ピリオド以外の記号をスペースに置換\n",
    "        for p in string.punctuation:\n",
    "            if (p == \".\") or (p == \",\"):\n",
    "                continue\n",
    "            else:\n",
    "                text = text.replace(p, \" \")\n",
    "\n",
    "        # ピリオドなどの前後にはスペースを入れておく\n",
    "        text = text.replace(\".\", \" . \")\n",
    "        text = text.replace(\",\", \" , \")\n",
    "        text = text.replace(\"・\", \" ・ \")\n",
    "        text = text.replace(\"、\", \"　、　\")\n",
    "        text = text.replace(\"。\", \"　。　\")\n",
    "        text = text.replace(\"「\", \" 　「　\")\n",
    "        text = text.replace(\"」\", \" 　」　\")\n",
    "        text = text.replace(\"【\", \"　【　\")\n",
    "        text = text.replace(\"】\", \"　】　\")\n",
    "        text = text.replace('\"', '　\"　')\n",
    "        text = text.replace(\"'\", \"　'　\")\n",
    "        return text\n",
    "\n",
    "    # 分かち書き\n",
    "    def tokenizer_punctuation(text):\n",
    "        text = normalize(text)\n",
    "        text = preprocessing_text(text)\n",
    "        return text\n",
    "\n",
    "\n",
    "    # 前処理と分かち書きをまとめた関数を定義\n",
    "    def tokenizer_with_preprocessing(text):\n",
    "        text = preprocessing_text(text)\n",
    "        ret = tokenizer_punctuation(text)\n",
    "        #mecabで分かち書き\n",
    "        ret = mecab.parse(str(ret))\n",
    "        ret = ret.strip().split()\n",
    "        return ret    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # データを読み込んだときに、読み込んだ内容に対して行う処理を定義します\n",
    "    # max_length\n",
    "    TEXT = torchtext.data.Field(sequential=True, tokenize=tokenizer_with_preprocessing, use_vocab=True,\n",
    "                                lower=True, include_lengths=True, batch_first=True, fix_length=100, init_token=\"<cls>\", eos_token=\"<eos>\")\n",
    "    LABEL = torchtext.data.Field(sequential=True, tokenize=tokenizer_with_preprocessing, use_vocab=True,\n",
    "                                lower=True, include_lengths=True, batch_first=True, fix_length=100, unk_token=\"<unk>\")\n",
    "\n",
    "    # フォルダ「data」から各tsvファイルを読み込みます\n",
    "    train_val_ds, test_ds = torchtext.data.TabularDataset.splits(\n",
    "        path='./train_data', train='Text.tsv',\n",
    "        test='Text.tsv', format='tsv',\n",
    "        fields=[('Text', TEXT), ('Label', LABEL)])\n",
    "\n",
    "    # torchtext.data.Datasetのsplit関数で訓練データとvalidationデータを分ける\n",
    "    train_ds, val_ds = train_val_ds.split(\n",
    "        split_ratio=0.8, random_state=random.seed(1234))\n",
    "\n",
    "    # torchtextで単語ベクトルとして読み込む(日本語)\n",
    "#     japanese_fasttext_vectors = Vectors(name = \".vec\")\n",
    "\n",
    "    # ベクトル化したバージョンのボキャブラリーを作成します\n",
    "    TEXT.build_vocab(train_ds, min_freq=1)\n",
    "    LABEL.build_vocab(train_ds, min_freq=1)\n",
    "    \n",
    "    # DataLoaderを作成します（torchtextの文脈では単純にiteraterと呼ばれています）\n",
    "    train_dl = torchtext.data.Iterator(\n",
    "        train_ds, batch_size=batch_size, train=True)\n",
    "\n",
    "    val_dl = torchtext.data.Iterator(\n",
    "        val_ds, batch_size=batch_size, train=False, sort=False)\n",
    "\n",
    "    test_dl = torchtext.data.Iterator(\n",
    "        test_ds, batch_size=batch_size, train=False, sort=False)\n",
    "\n",
    "    return train_dl, val_dl, test_dl, TEXT, LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./catboost_info',\n",
       " './test.csv',\n",
       " './Attention_LSTM_LCM.ipynb',\n",
       " './train.dat',\n",
       " './Untitled2.ipynb',\n",
       " './README.md',\n",
       " './Untitled1.ipynb',\n",
       " './train.csv',\n",
       " './data.csv',\n",
       " './voc.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(\"./*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, val_dl, test_dl, TEXT, LABEL = get_DataLoaders_and_TEXT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " dataloders_dict = {\"train\": train_dl, \"val\": val_dl}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b10430241691b249f3f5968ff1d5f373841e9f0f1c6e818b42478bb30bce80ad"
  },
  "kernelspec": {
   "display_name": "Python 3.6.6 64-bit ('docker-webapi': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
